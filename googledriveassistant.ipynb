{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c758870b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core dependencies\n",
    "#%pip install python-dotenv\n",
    "\n",
    "# LangChain ecosystem\n",
    "#%pip install langchain\n",
    "#%pip install langchain-openai\n",
    "#%pip install langchain-chroma\n",
    "#%pip install langchain-google-community[drive]\n",
    "\n",
    "# Vector store\n",
    "#%pip install chromadb\n",
    "\n",
    "# LLM APIs\n",
    "#%pip install openai\n",
    "#%pip install anthropic\n",
    "\n",
    "# LangGraph\n",
    "#%pip install langgraph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474345cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# LangChain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_google_community import GoogleDriveLoader\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "# LangGraph\n",
    "from langchain_core.tools import create_retriever_tool\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langgraph.graph import StateGraph, MessagesState, START, END\n",
    "from langchain_core.messages import SystemMessage\n",
    "\n",
    "# Typing\n",
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2ef728",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95df4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'your-key-if-not-using-env')\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = \"credentials.json\"\n",
    "os.environ['LANGCHAIN_API_KEY'] = os.getenv('LANGCHAIN_API_KEY', 'your-key-if-not-using-env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0988fbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding delay time between extractions so we don't surpass the Googles API Quota limit\n",
    "original_load_sheet = GoogleDriveLoader._load_sheet_from_id\n",
    "def load_sheet_with_delay(self, sheet_id):\n",
    "    print(f\"Loading sheet: {sheet_id}\")\n",
    "    time.sleep(2)  # Wait 2 seconds between each sheet\n",
    "    return original_load_sheet(self, sheet_id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9058857",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = GoogleDriveLoader(\n",
    "    folder_id=\"root\",\n",
    "    credentials_path=\"credentials.json\",\n",
    "    token_path=\"token.json\",\n",
    "    recursive=True,\n",
    "    file_types=[\"pdf\",\"sheets\",\"documents\"],\n",
    "    scopes=['https://www.googleapis.com/auth/drive.readonly']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f34746",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = loader.load()\n",
    "print(f\"Loaded {len(documents)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519d028b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77902f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if 'documents' variable exists and has content\n",
    "try:\n",
    "    print(f\"Documents in memory: {len(documents)} documents\")\n",
    "except NameError:\n",
    "    print(\"'documents' variable not found - need to reload or restore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b663ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is because i had to load the docuemnt in a separated way\n",
    "all_documents = documents + sheets_documents + pdf_documents\n",
    "print(f\"Total documents: {len(all_documents)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93eec2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200\n",
    ")\n",
    "split_docs = text_splitter.split_documents(documents)\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=split_docs,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=\".......................¨\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac487937",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma(\n",
    "    persist_directory=\".......................¨,\n",
    "    embedding_function=OpenAIEmbeddings()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ba5a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 12}  # Return top 12 most relevant chunks, why ? bc yeah\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2da9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################## NOW WE CREATE A LANGGRAPH SYSTEM, THAT  USES CLAUDE(ANTRHOPIC) ##########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f215f2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###New Libraries needed for langgraph\n",
    "from langchain_core.tools import create_retriever_tool\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.graph import MessagesState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7204da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.chat_models import init_chat_model\n",
    "load_dotenv(override=True)\n",
    "\n",
    "os.environ[\"ANTHROPIC_API_KEY\"] = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "llm = init_chat_model(\"anthropic:claude-3-5-sonnet-latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4408f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "##CREATE A STATE GRAPH -> NOW EACH NODE CAN RECIVE THE CURRENT STATE AS INPUT AND OUTPUT AND UPDATE TO THE STATE\n",
    "#Updates to messages will be appended to the existing list rather than overwritingit, thanks to prebuilt reducer function.\n",
    "from typing import Annotated\n",
    "\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    # Messages have the type \"list\". The `add_messages` function\n",
    "    # in the annotation defines how this state key should be updated\n",
    "    # (in this case, it appends messages to the list, rather than overwriting them)\n",
    "    messages: Annotated[list, add_messages]\n",
    "    retrieved_docs: list  \n",
    "\n",
    "\n",
    "graph_builder = StateGraph(State)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b9f40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatbot(state: State):\n",
    "    return {\"messages\": [llm.invoke(state[\"messages\"])]}\n",
    "\n",
    "\n",
    "# The first argument is the unique node name\n",
    "# The second argument is the function or object that will be called whenever\n",
    "# the node is used.\n",
    "graph_builder.add_node(\"chatbot\", chatbot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905b2c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a949c1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_builder.add_edge(START, \"chatbot\")\n",
    "graph_builder.add_edge(\"chatbot\", END)\n",
    "graph = graph_builder.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5103728",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0caf646d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_graph_updates(user_input: str):\n",
    "    for event in graph.stream({\"messages\": [{\"role\": \"user\", \"content\": user_input}]}):\n",
    "        for value in event.values():\n",
    "            print(\"Assistant:\", value[\"messages\"][-1].content)\n",
    "\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        user_input = input(\"User: \")\n",
    "        if user_input.lower() in [\"quit\", \"exit\", \"q\"]:\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "        stream_graph_updates(user_input)\n",
    "    except:\n",
    "        # fallback if input() is not available\n",
    "        user_input = \"What do you know about LangGraph?\"\n",
    "        print(\"User: \" + user_input)\n",
    "        stream_graph_updates(user_input)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbbe725",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "# UPDATED IMPORT - changed from langchain.tools.retriever\n",
    "from langchain_core.tools import create_retriever_tool\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langgraph.graph import StateGraph, MessagesState, START, END\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.messages import SystemMessage\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(override=True)\n",
    "os.environ[\"ANTHROPIC_API_KEY\"] = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "\n",
    "# Initialize LLM\n",
    "llm = init_chat_model(\"claude-3-5-sonnet-20241022\", model_provider=\"anthropic\")\n",
    "\n",
    "# Create retriever tool using your existing retriever\n",
    "retriever_tool = create_retriever_tool(\n",
    "    retriever,  # Your existing retriever object goes here\n",
    "    name=\"retrieve_documents\", \n",
    "    description=\"Search and return relevant information from the document collection to answer questions.\"\n",
    ")\n",
    "\n",
    "# LLM with tools bound\n",
    "llm_with_tools = llm.bind_tools([retriever_tool])\n",
    "\n",
    "# SYSTEM MESSAGE FOR MARKDOWN FORMATTING\n",
    "MARKDOWN_SYSTEM_MESSAGE = SystemMessage(content=\"\"\"You are a helpful AI assistant that always responds in well-formatted Markdown.\n",
    "\n",
    "Follow these formatting guidelines:\n",
    "- Use **bold** for important terms and concepts\n",
    "- Use *italics* for emphasis\n",
    "- Use `code blocks` for technical terms, file names, or code\n",
    "- Use bullet points with - or * for lists\n",
    "- Use ## for section headers when organizing longer responses\n",
    "- Use > for quotes or important notes\n",
    "- Use numbered lists (1., 2., 3.) for step-by-step instructions\n",
    "- When citing sources, use proper markdown links if available\n",
    "\n",
    "Always make your responses clear, well-structured, and visually appealing using markdown formatting.\"\"\")\n",
    "\n",
    "def query_or_respond(state: MessagesState):\n",
    "    \"\"\"LLM decides whether to retrieve or respond directly.\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    \n",
    "    # Add system message if this is the start of conversation\n",
    "    if not any(isinstance(msg, SystemMessage) for msg in messages):\n",
    "        messages = [MARKDOWN_SYSTEM_MESSAGE] + messages\n",
    "\n",
    "    response = llm_with_tools.invoke(state[\"messages\"])\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "def generate_response(state: MessagesState):\n",
    "    \"\"\"Generate final response using retrieved context.\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    # Ensure system message is present\n",
    "    if not any(isinstance(msg, SystemMessage) for msg in messages):\n",
    "        messages = [MARKDOWN_SYSTEM_MESSAGE] + messages\n",
    "\n",
    "    response = llm.invoke(state[\"messages\"])\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "# Create ToolNode for automatic tool execution\n",
    "tools = ToolNode([retriever_tool])\n",
    "\n",
    "# Build the graph\n",
    "graph_builder = StateGraph(MessagesState)\n",
    "graph_builder.add_node(\"query_or_respond\", query_or_respond)\n",
    "graph_builder.add_node(\"retrieve\", tools)\n",
    "graph_builder.add_node(\"generate\", generate_response)\n",
    "\n",
    "# Add conditional edges\n",
    "graph_builder.add_edge(START, \"query_or_respond\")\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"query_or_respond\",\n",
    "    tools_condition,\n",
    "    {\n",
    "        \"tools\": \"retrieve\",\n",
    "        END: END,\n",
    "    }\n",
    ")\n",
    "graph_builder.add_edge(\"retrieve\", \"generate\")\n",
    "graph_builder.add_edge(\"generate\", END)\n",
    "\n",
    "# Compile graph\n",
    "graph = graph_builder.compile()\n",
    "\n",
    "# Streaming function\n",
    "def stream_graph_updates(user_input: str):\n",
    "    \"\"\"\n",
    "    Ch\"\"\"\n",
    "    for event in graph.stream({\"messages\": [{\"role\": \"user\", \"content\": user_input}]}):\n",
    "        for value in event.values():\n",
    "            if \"messages\" in value and value[\"messages\"]:\n",
    "                last_message = value[\"messages\"][-1]\n",
    "                if hasattr(last_message, 'content'):\n",
    "                    print(\"Assistant:\", last_message.content)\n",
    "                    print()\n",
    "\n",
    "# Main loop\n",
    "while True:\n",
    "    try:\n",
    "        user_input = input(\"User: \")\n",
    "        if user_input.lower() in [\"quit\", \"exit\", \"q\"]:\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "        stream_graph_updates(user_input)\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nGoodbye!\")\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
